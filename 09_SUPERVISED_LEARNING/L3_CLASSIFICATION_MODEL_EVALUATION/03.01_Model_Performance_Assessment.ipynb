{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02dec93-9e19-40d0-928c-f5598483428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRODUCTION TO EVALUATING CLASSIFICATION MODELS\n",
    "# Imagine the following scenario:\n",
    "# A credit card company wants to detect fraudulent transactions in real-time.\n",
    "# Historically, 10 of every 100,000 transactions have been fraudulent.\n",
    "# An analyst writes a program to detect fraudulten transactions.\n",
    "# But because of a bug, it flags every transaction as not fraudulent.\n",
    "# So fore every 100,000 transactions, it correctly classifies 99,990 transactions that aren't fraudulent, and it erroneously classifies the 10 fraudulent transactions.\n",
    "# The program's accuracy score appears impressive at 99.99%.\n",
    "# However, it spectacularly fails at its job - detecting 0 of every 10 fraudulent transactions.\n",
    "# The success rate is therefore 0%.\n",
    "# To catch problems like this, we need additional evaluation metrics.\n",
    "# In this lesson, you'll learn how to calculate and apply advanced evaluation metrics to evaluate the performance of your classification models.\n",
    "# Previous lessons covered the model-fit-predict pattern.\n",
    "# This lesson adds another stage to our machine learning pattern, giving us model-fit-predict-evaluate.\n",
    "# After identifying the right model, we fit our data ot it. \n",
    "# We can then use the model to make predictions about new data.\n",
    "# Finally, we evaluate how well those predictions performed.\n",
    "# If the last stage shows that the model didn't make predictions well, we might select a different machine learning model.\n",
    "# By evaluating additional metrics for model performance in this lesson, you'll sharpen your skills regarding the last stage of this machine learning pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21a2d0e-9cb0-484e-9ee3-f6b571fbd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSESSING MODEL PERFORMANCE\n",
    "# It's often not enough to train and then use a machine learning model for making predictions.\n",
    "# We also  need to know how well the model performs at its prediction task.\n",
    "# In the previous lesson, we learned one way of assessing performance - by using the accuracy score.\n",
    "# But, we also want to know the percentage of predictions that the model gets right and how well it predicts each outcome.\n",
    "# To help answer these questions, we can use the following four metrics to give us additional insight into the model's performance:\n",
    "    # 1. Accuracy\n",
    "    # 2. Precision\n",
    "    # 3. Recall\n",
    "    # 4. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3a618e-f881-4cd8-93eb-a432cc399e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY\n",
    "# Earlier, we learned that we can measure a model's performance based on the differences between its predicted and its actual values.\n",
    "# We examined the accuracy score to summarize this performance.\n",
    "# In this section, we'll define and explain accuracy in more detail.\n",
    "# A classification algorithm results in tow or more outcomes.\n",
    "# Classifying fraudulent transactions, for example, results in two outcomes: A transaction is fraudulent or not.\n",
    "# We can categorize these two predictions according to a confusion matrix.\n",
    "# A confusion matrix groups our model's predictions according to whether the model accurately predicted categories.\n",
    "# Or did it confuse any predictions with the wrong categories?\n",
    "# By comparing the accurate predictons to the inaccurate (confused) predictions, a confusion matrix supplies a way for us to evaluate the model's performance.\n",
    "# It contains the number of times that a model correctly predicted a positive or negative value and the number of times that it didn't.\n",
    "\n",
    "# CONFUSION MATRIX\n",
    "# A confusion matrix consist of two rows and two columns:\n",
    "# The two columns are the 'Predicted True' and 'Predicted False' columns, respectively.\n",
    "# The two rows are the 'Actually True' and 'Actually False' columns, respectively.\n",
    "# Within the first column of the matrix are the positive predictions: So the 'True Positives' and 'False Positives'.\n",
    "# This first column separates all the predictions that the model made for the positive class into whether they were accurate or not. (In our example, the positive class is the 'fraudulent' class).\n",
    "# The second column contains all the negative predictions - the 'False Negatives' and 'True Negatives'.\n",
    "# It separates all the predictions that the model made for the negative class. (In our example, the negative class is the 'not fraudulent' class).\n",
    "# To further explain, any prediction falls into one of the two categories: true or false.\n",
    "# In the context of fraud detection, a true prediction means the model categorized the transaction as fraudulent.\n",
    "# A false prediction means that the model categorized the transaction as not fraudulent.\n",
    "# If the model predicted a transaction as fraudulent, and the transaction really was fraudulent, we call that prediction a TRUE POSITIVE (TP).\n",
    "# If the model predicted a transaction as fraudulent, but the transaction wasn't fraud, we call that prediction a FALSE POSITIVE (FP).\n",
    "# If the model predicted a transaction as not fraud, but it was fraud, we call the prediction a FALSE NEGATIVE (FN).\n",
    "# Lastly, if the model predicted a transaction as not fraud, and it wasn't fraud, we call this prediction a TRUE NEGATIVE (TN).\n",
    "# We can use all this information to calculate the accuracy of the model.\n",
    "# The ACCURACY measures how often the model was correct. \n",
    "# It does so by calculating the ratio of the number of correct predictions to the total number of outcomes.\n",
    "# The formula for accuracy is as follows:\n",
    "    # accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "# Now, we can manually calculate the accuracy each time.\n",
    "# However, FinTech professionals always seek a shortcut to a problem.\n",
    "# In this case, we can continue to use the `accuracy_score` function to estimate the accuracy of the model.\n",
    "# The next metric that we want to explore is the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6620ed95-6fe9-4d7c-9d72-0a212e034bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRECISION\n",
    "# The precision metric relates to the accuracy metric but slightly differs.\n",
    "# As with the accuracy, one way to illustrate the concept of precision is through a confusion matrix.\n",
    "# To build a confusion matrix, you compare a list of the known values with a list of the values the model predicted.\n",
    "# For example, consider a confusion matrix that demonstrates the performance of a model on the fraudulent transaction dataset.\n",
    "# This confusion matrix shows the comparison between the actual and the predicted values.\n",
    "# But, it splits these values according to whether the predictions were positive (true, predicting fraud).\n",
    "# Or if the predictions were negative (false, predicting no fraud).\n",
    "# Let's say, in our case, the intersection of 'Actually True' and 'Predicted True' shows that the model predicted 30 TPs.\n",
    "# That is, of all the predictions that the model predicted as fraudulent, 30 of them actually were.\n",
    "# This gets us to the precision.\n",
    "# PRECISION, also known as the POSITIVE PREDICTIVE VALUE (PPV), measures how confident we are that the model correctly made the positive predictions.\n",
    "# We get the precision by dividing the number of TPs by the number of all the positives. (The latter is the sum of the TPs and the FPs).\n",
    "# The formula for precision is as follows:\n",
    "    # precision = TPs / (TPs + FPs)\n",
    "# Let's say that aside from the 30 TPs, 20 FPs were predicted, all of which turned out to be legitimate transactions.\n",
    "# The precision would therefore be the following calculation:\n",
    "    # precision = 30 / (30 + 20) or 30 / 50 = 0.6\n",
    "# Note that we use the cells in the 'Predicted true' column to calculate the precision.\n",
    "# To summarize, in machine learning, the precision measures the reliability of positive classification.\n",
    "# Asking yourself the following question might help you remember how to think about the precision:\n",
    "    # I know the model just predicted a fraudulent transaction, but, how likely is it that the transaction is actually fraudulent?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca4c121-ab66-4ec3-b388-6d241ca3b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALL\n",
    "# Another way that we can assess a model's performance is by using the recall, which is also called the sensitivity.\n",
    "# People in machine learning more commonly use the term recall.\n",
    "# In our example, the RECALL measures that number of actually fraudulent transaction that the model correctly classified as fraudulent.\n",
    "# To understand the recall, ask yourself the following question:\n",
    "    # I know that this transaction is fraudulent, but how likely is it that the model will predict it as fraudulent?\n",
    "# The formula for the recall is as follows:\n",
    "    # recall = TPs / (TPs + FNs)\n",
    "# To get the recall, we start with the number of TPs - that is, the number of times that the model correctly predicted a fraudulent transaction.\n",
    "# We then compare this number to the total number of actually fraudulent transactions - including the ones that the model missed (that is, the FNs).\n",
    "# Let's explain this by using the same confusion matrix as before.\n",
    "# Let's say in our hypothetical confusion matrix, we have the 30 TPs and we have 10 FNs, which highlights the Actually true row.\n",
    "# The recall is thus as follows:\n",
    "    # recall = 30 / (30 + 10) | 30 / 40 = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11cae039-c6ac-41da-8dff-a47bef4c559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT\n",
    "# A fundamental tension exists between the precision and the racall.\n",
    "# Highly sensitive tests and algorithms tend to be aggressive.\n",
    "# This is because they effectively detect the intended targets. \n",
    "# But, this means that they also risk resulting in numerous false positives.\n",
    "# High precision, by contrast, usually results from a conservative process.\n",
    "# In this case, the predicted positives are likely actual positives. \n",
    "# But, the model might not predict numerous other true positives. \n",
    "# In practice, we need to make a tradeoff between the recall and the precision that requires a balancing act between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35fb78eb-6701-4296-82ac-894746927fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 SCORE\n",
    "# We can characterize the F1 SCORE which is also called the HARMONIC MEAN, as a single summary statistic for the precison and the recall.\n",
    "# The formula for the F1 score is as follows:\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "# The F1 score for our fraudulent transaction classifier is thus as follows:\n",
    "    # F1 = 2 * (0.6 * 0.75) / (0.6 + 0.75) == 0.66\n",
    "# To illustrate the F1 score, say that 10 transactions out of 10,000 are fraudulent.\n",
    "# What does the F1 score tell us in this scenario?\n",
    "# Note that a large class imbalance exists between the transactions that are fraudulent and those that aren't.\n",
    "# That is, we have much fewer transactions that are fraudulent than those that aren't.\n",
    "# Our model might have a high F1 score, but that can prove deceptive, because the model might do a good job of predicting only our larger class.\n",
    "# The larger class consist of 9,990 transactions that aren't fraudulent.\n",
    "# However, we're interested in the smaller class - that is the fraudulent transactions that the model correctly identified.\n",
    "# Consider a model that we trained for such a dataset.\n",
    "# The precision of this model is 0.99039 (9900 / 9906).\n",
    "# The recall is 0.99099 (9900 / 9990).\n",
    "# Using the F1 score, which is, 2 * (0.99039 * 0.99099) / (0.99039 + 0.99099), we arrive at an F1 score of 0.995.\n",
    "# An F1 score of 1 is considered perfect, so in this case, the model performed well.\n",
    "# In fact, this model excels at predicting the larger class - but we're not interested in that.\n",
    "# The model correctly predicted the fraudulent transaction only 40% of the time (4 TNs / (6 FPs + 4 TNs))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c77e3c-ebc0-4e86-83bf-c657083fe2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ON THE JOB\n",
    "# If you want to discuss the predictive power of your model, you need to make sure that it will do as you claim.\n",
    "# You need empirical data to show that your model does whay you say it does. \n",
    "# Evaluating the model's performance and explaining its limitations is fundamental to instilling confidence in a team of coworkers that your model works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
