{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0742f7-5834-4a2b-8c5c-a949850d027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependecies:\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.python as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478574a5-b600-40c5-b601-4d11810b8268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATING DEEP LEARNING MODELS WITH KERAS\n",
    "# In the previous lesson, we learned how to build a neural network model with a single hidden layer using Keras.\n",
    "# To create a deep learning model, we'll build on this knowledge.\n",
    "# Suppose that we've been hired by a Spanish winery in the Rioja region.\n",
    "# The winery sees new market opportunities in Australia and wants to expand.\n",
    "# To forecast future revenue, the winery owners want to predict the quality of wine in future wine lots.\n",
    "# We will build a deep learning neural network model to help them predict the quality scores of different wines.\n",
    "# As usual, we'll start by reading the data into a DataFrame, as shown in the following code:\n",
    "\n",
    "# Create the winery DataFrame:\n",
    "wine_df = pd.read_csv(\n",
    "    Path('wine_quality.csv')\n",
    ")\n",
    "\n",
    "# Review the first and last 5 rows of data:\n",
    "display(wine_df.head())\n",
    "display(wine_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a413003-cb76-468b-be62-6a2fc90d157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKDOWN\n",
    "# Let's take a closer look at the resulting DataFrame.\n",
    "# The data contains 11 variables representing different wine characteristics.\n",
    "# Together, these characteristics allow us to assess the overall 'quality' of a wine on a scale from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baaa5977-86d6-4a63-8b87-dc74da309911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS THE DATA\n",
    "# Next, we must preprocess our data.\n",
    "# We create the features set (X) and the target set (y).\n",
    "# The following code creates these datasets:\n",
    "\n",
    "# Create the features (X) and the target (y):\n",
    "X = wine_df.drop(columns=['quality']).values\n",
    "y= wine_df['quality'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd6f649-0886-4a5e-91b7-60607e6a0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# The data used to fit a neural network should always be numerical and normalized to the same scale.\n",
    "# This is true regardless of how many hidden layers the neural network contains.\n",
    "\n",
    "# The following code creates the training and testing datasets, and scales the data:\n",
    "\n",
    "# Create the training and testing datasets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Create the scaler instance:\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fith the scaler:\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the features data:\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bca1cb2-3699-4b4d-b3b1-34a47577ba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\legar\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MODEL THE DEEP NEURAL NET WITH KERAS\n",
    "# To define a deep neural net with Keras, we will use a process similar to the one we used to define a simple neural network.\n",
    "# But, this time, we will add an additional hidden layer.\n",
    "# In this demonstration, the second hidden layer will contain fewer neurons than the first.\n",
    "# This is typically how deep neural nets are constructed: The number of neurons on each successive layer is equal to or less than the number of neurons on the previous layer, with the output layer contain the fewest neurons.\n",
    "# As we did for our previous neural network, we will choose an activation function for the first layer.\n",
    "# This time, we will also use this same activation function for our second hidden layer.\n",
    "# Often, developers experiment with many potential architectures in an effort to minimize the loss metric.\n",
    "\n",
    "# CONNECT THE DOTS\n",
    "# As we build this model, you may notice that we continue to follow a process similar to the one we previously used to build a simple neural network.\n",
    "# Later in the less, we will compile our model, and then we'll define a loss function.\n",
    "# As in the previous model, we will seek to minimize the metric that the loss function returns.\n",
    "\n",
    "# The following code creates our deep learning model and adds two hidden layers to the model:\n",
    "\n",
    "# Define the model - deep neural net with two hidden layers:\n",
    "input_features = 11\n",
    "hidden_nodes_layer1 = 8\n",
    "hidden_nodes_layer2 = 4\n",
    "\n",
    "# Create a sequential neural network model:\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the first hidden layer:\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=input_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer:\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "\n",
    "# Add the output layer:\n",
    "nn.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30661499-aaf2-44e2-b7ce-25ce284d7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKDOWN\n",
    "# To add a second layer to a neural network, all you have to do is include another call to the `add` function.\n",
    "# Note that on the second hidden layer, we do not define an `input_dim` parameter.\n",
    "# We only define the number of neurons the layer will contain (using the `units` parameter) and the activation function.\n",
    "# In this case, we use the same activation function (ReLU) for the first and second layers.\n",
    "# Then, for the output layer, we use the linear activation function.\n",
    "# The linear activation function allows for the multiple outputs we need to our 1-10 wine quality scale (rather htan just a binary 0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0dae645-0740-4f57-b300-21dad924f0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\legar\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\legar\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\legar\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "38/38 [==============================] - 1s 3ms/step - loss: 27.4295 - mse: 27.4295\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 23.4652 - mse: 23.4652\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 19.3266 - mse: 19.3266\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 15.0453 - mse: 15.0453\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 10.7488 - mse: 10.7488\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 6.9672 - mse: 6.9672\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 4.3959 - mse: 4.3959\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 3.1064 - mse: 3.1064\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 2.5615 - mse: 2.5615\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 2.2754 - mse: 2.2754\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 2.0758 - mse: 2.0758\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.9141 - mse: 1.9141\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.7840 - mse: 1.7840\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 1.6649 - mse: 1.6649\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.5707 - mse: 1.5707\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.4834 - mse: 1.4834\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.4064 - mse: 1.4064\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.3350 - mse: 1.3350\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 1.2656 - mse: 1.2656\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 1.2045 - mse: 1.2045\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 1.1469 - mse: 1.1469\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.0947 - mse: 1.0947\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.0516 - mse: 1.0516\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 1.0045 - mse: 1.0045\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.9628 - mse: 0.9628\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.9289 - mse: 0.9289\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.8945 - mse: 0.8945\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.8621 - mse: 0.8621\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.8344 - mse: 0.8344\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.8054 - mse: 0.8054\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.7809 - mse: 0.7809\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.7537 - mse: 0.7537\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.7324 - mse: 0.7324\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.7104 - mse: 0.7104\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6906 - mse: 0.6906\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6729 - mse: 0.6729\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6569 - mse: 0.6569\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6426 - mse: 0.6426\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6264 - mse: 0.6264\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6149 - mse: 0.6149\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6013 - mse: 0.6013\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5900 - mse: 0.5900\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5777 - mse: 0.5777\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5672 - mse: 0.5672\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5574 - mse: 0.5574\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5471 - mse: 0.5471\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5403 - mse: 0.5403\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5320 - mse: 0.5320\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5247 - mse: 0.5247\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5204 - mse: 0.5204\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5090 - mse: 0.5090\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5035 - mse: 0.5035\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4991 - mse: 0.4991\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4929 - mse: 0.4929\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4869 - mse: 0.4869\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4821 - mse: 0.4821\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4783 - mse: 0.4783\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4751 - mse: 0.4751\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4693 - mse: 0.4693\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4681 - mse: 0.4681\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4624 - mse: 0.4624\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4602 - mse: 0.4602\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4555 - mse: 0.4555\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4527 - mse: 0.4527\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4509 - mse: 0.4509\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4489 - mse: 0.4489\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4444 - mse: 0.4444\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4433 - mse: 0.4433\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4410 - mse: 0.4410\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4398 - mse: 0.4398\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4366 - mse: 0.4366\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4356 - mse: 0.4356\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4311 - mse: 0.4311\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4323 - mse: 0.4323\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4319 - mse: 0.4319\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4262 - mse: 0.4262\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4254 - mse: 0.4254\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4247 - mse: 0.4247\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4227 - mse: 0.4227\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4204 - mse: 0.4204\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4190 - mse: 0.4190\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4182 - mse: 0.4182\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4146 - mse: 0.4146\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4138 - mse: 0.4138\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4141 - mse: 0.4141\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4122 - mse: 0.4122\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4117 - mse: 0.4117\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4092 - mse: 0.4092\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4100 - mse: 0.4100\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4083 - mse: 0.4083\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4067 - mse: 0.4067\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4080 - mse: 0.4080\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4041 - mse: 0.4041\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4033 - mse: 0.4033\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3985 - mse: 0.3985\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3992 - mse: 0.3992\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3997 - mse: 0.3997\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3955 - mse: 0.3955\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3960 - mse: 0.3960\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3978 - mse: 0.3978\n"
     ]
    }
   ],
   "source": [
    "# COMPILE AND FIT THE MODEL\n",
    "# Now we will compile and fit our deep neural network model.\n",
    "# The following code compiles and fits the model:\n",
    "\n",
    "# Compile the model:\n",
    "nn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "# Fit the model:\n",
    "deep_net_model = nn.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c41d5e2-f05e-41b4-9944-76b919281ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REWIND\n",
    "# Recall that epochs can be loosely defined as one iteration of a neural network model.\n",
    "# That is, one pass of the entire training dataset through the model.\n",
    "# The loss function scores the model's performance after each new epoch.\n",
    "# The optimizer shapes and molds the model while it's trained on the data.\n",
    "# And finally, the evaluation metric assesses the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb8151bd-9ff2-4d28-acbc-adfdf3157165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKDOWN\n",
    "# Let's take a closer look at the parameters we chose to compile this model.\n",
    "# This model will output numbers (wine quality scores) ranging from 1 to 10.\n",
    "# This means that the model's output is continuous, rather than binary.\n",
    "# So, we're building a regression model, rather than a classification model.\n",
    "# We used the `mean_squared_error` loss function, which is designed for regression problems.\n",
    "# We also used the MSE metric to evaluate the quality of the model.\n",
    "# Remember that there are two main evaluation metrics:\n",
    "    # 1. Model Predictive Accuracy\n",
    "    # 2. Modedl Mean Squared Error (MSE)\n",
    "# We use accuracy for classification models and MSE for regression models.\n",
    "# When using MSE to assess a model, an MSE value closer to zero indicates a better model.\n",
    "# So, in this case, we want our wine-quality prediction model to return an MSE value close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5f1d604-c4bd-4cc3-91e2-588cfb2aeea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 1s 2ms/step - loss: 27.0499 - mse: 27.0499\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 17.6314 - mse: 17.6314\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 9.3104 - mse: 9.3104\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 4.6828 - mse: 4.6828\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 3.0185 - mse: 3.0185\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 2.3610 - mse: 2.3610\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 2.0238 - mse: 2.0238\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.7955 - mse: 1.7955\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.6259 - mse: 1.6259\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.5025 - mse: 1.5025\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.3850 - mse: 1.3850\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.2960 - mse: 1.2960\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.2189 - mse: 1.2189\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.1483 - mse: 1.1483\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.0872 - mse: 1.0872\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 1.0311 - mse: 1.0311\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.9852 - mse: 0.9852\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.9419 - mse: 0.9419\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.8971 - mse: 0.8971\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.8646 - mse: 0.8646\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.8261 - mse: 0.8261\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.7941 - mse: 0.7941\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.7636 - mse: 0.7636\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.7328 - mse: 0.7328\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.7079 - mse: 0.7079\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6863 - mse: 0.6863\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6632 - mse: 0.6632\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6395 - mse: 0.6395\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6278 - mse: 0.6278\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6041 - mse: 0.6041\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5865 - mse: 0.5865\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5723 - mse: 0.5723\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5572 - mse: 0.5572\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5449 - mse: 0.5449\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5332 - mse: 0.5332\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5240 - mse: 0.5240\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5067 - mse: 0.5067\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.5051 - mse: 0.5051\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4914 - mse: 0.4914\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4863 - mse: 0.4863\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4760 - mse: 0.4760\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4664 - mse: 0.4664\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4636 - mse: 0.4636\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4560 - mse: 0.4560\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4488 - mse: 0.4488\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4459 - mse: 0.4459\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4428 - mse: 0.4428\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4338 - mse: 0.4338\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4292 - mse: 0.4292\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4249 - mse: 0.4249\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4232 - mse: 0.4232\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4201 - mse: 0.4201\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4150 - mse: 0.4150\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4133 - mse: 0.4133\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4104 - mse: 0.4104\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4042 - mse: 0.4042\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4026 - mse: 0.4026\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.4013 - mse: 0.4013\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3943 - mse: 0.3943\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3968 - mse: 0.3968\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3892 - mse: 0.3892\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3858 - mse: 0.3858\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3893 - mse: 0.3893\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3858 - mse: 0.3858\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3825 - mse: 0.3825\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3841 - mse: 0.3841\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3762 - mse: 0.3762\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3750 - mse: 0.3750\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3711 - mse: 0.3711\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3686 - mse: 0.3686\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3658 - mse: 0.3658\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3662 - mse: 0.3662\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3709 - mse: 0.3709\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3639 - mse: 0.3639\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3619 - mse: 0.3619\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3605 - mse: 0.3605\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3576 - mse: 0.3576\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3557 - mse: 0.3557\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3534 - mse: 0.3534\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.3534 - mse: 0.3534\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3520 - mse: 0.3520\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3502 - mse: 0.3502\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3449 - mse: 0.3449\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3501 - mse: 0.3501\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3463 - mse: 0.3463\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3443 - mse: 0.3443\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3419 - mse: 0.3419\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3418 - mse: 0.3418\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3429 - mse: 0.3429\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3402 - mse: 0.3402\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3391 - mse: 0.3391\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3399 - mse: 0.3399\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3370 - mse: 0.3370\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3354 - mse: 0.3354\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3331 - mse: 0.3331\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3353 - mse: 0.3353\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3360 - mse: 0.3360\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3335 - mse: 0.3335\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3314 - mse: 0.3314\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3303 - mse: 0.3303\n"
     ]
    }
   ],
   "source": [
    "# DEFINING THE NUMBER OF HIDDEN LAYERS\n",
    "# When designing a deep learning model, we have to decide: How many hidden layers should we add to the neural network?\n",
    "# Usually, adding more layers to a model increases the model's overall performance - but sometimes it doesn't.\n",
    "# That's because sometimes, additinoal layers can become redundant - that is, the first hidden layers can sufficiently encapsulate the dataset's complexity.\n",
    "# In this section, we'll demonstrate the impact of adding more layers to a neural network model.\n",
    "# For this example, we'll use the wine quality dataset again.\n",
    "# However, this time, we'll define two different models.\n",
    "# Our first model will have 2 hidden layers.\n",
    "# We will use 22 neurons on the first layer and 11 neurons on the second layer.\n",
    "# The following code defines the model:\n",
    "\n",
    "# Define the model - deep neural net with two hidden layers:\n",
    "number_input_features = 11\n",
    "hidden_nodes_layer_1 = 22\n",
    "hidden_nodes_layer_2 = 11\n",
    "\n",
    "# Create a sequential neural network model:\n",
    "nn_1 = Sequential()\n",
    "\n",
    "# Add the first hidden layer:\n",
    "nn_1.add(Dense(units=hidden_nodes_layer_1, input_dim=number_input_features, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer:\n",
    "nn_1.add(Dense(units=hidden_nodes_layer_2, activation='relu'))\n",
    "\n",
    "# Add the output layer:\n",
    "nn_1.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compile the model:\n",
    "nn_1.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "# Fit the model:\n",
    "deep_net_model_1 = nn_1.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e1346e-622e-4419-b1a4-6d8fa8243568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 2s 5ms/step - loss: 32.6906 - mse: 32.6906\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 26.0173 - mse: 26.0173\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 17.6544 - mse: 17.6544\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 8.3392 - mse: 8.3392\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 3.9431 - mse: 3.9431\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 2.8496 - mse: 2.8496\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.3244 - mse: 2.3244\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 2.0132 - mse: 2.0132\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 1.8031 - mse: 1.8031\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.6376 - mse: 1.6376\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.5156 - mse: 1.5156\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 1.4163 - mse: 1.4163\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.3310 - mse: 1.3310\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 1.2604 - mse: 1.2604\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 1.1882 - mse: 1.1882\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 1.1281 - mse: 1.1281\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.0704 - mse: 1.0704\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 1.0143 - mse: 1.0143\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.9674 - mse: 0.9674\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.9224 - mse: 0.9224\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.8817 - mse: 0.8817\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.8389 - mse: 0.8389\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.8036 - mse: 0.8036\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.7709 - mse: 0.7709\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.7406 - mse: 0.7406\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.7068 - mse: 0.7068\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.6817 - mse: 0.6817\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.6568 - mse: 0.6568\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.6357 - mse: 0.6357\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.6160 - mse: 0.6160\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5980 - mse: 0.5980\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5818 - mse: 0.5818\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5632 - mse: 0.5632\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5483 - mse: 0.5483\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5371 - mse: 0.5371\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5267 - mse: 0.5267\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5129 - mse: 0.5129\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.5030 - mse: 0.5030\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4899 - mse: 0.4899\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4826 - mse: 0.4826\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4726 - mse: 0.4726\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4649 - mse: 0.4649\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4576 - mse: 0.4576\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4476 - mse: 0.4476\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.4433 - mse: 0.4433\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4400 - mse: 0.4400\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4291 - mse: 0.4291\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4240 - mse: 0.4240\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4190 - mse: 0.4190\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4168 - mse: 0.4168\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4117 - mse: 0.4117\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4079 - mse: 0.4079\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4075 - mse: 0.4075\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4009 - mse: 0.4009\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3990 - mse: 0.3990\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3951 - mse: 0.3951\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3885 - mse: 0.3885\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3871 - mse: 0.3871\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3883 - mse: 0.3883\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3831 - mse: 0.3831\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3842 - mse: 0.3842\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3784 - mse: 0.3784\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3737 - mse: 0.3737\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3687 - mse: 0.3687\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3716 - mse: 0.3716\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3703 - mse: 0.3703\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3687 - mse: 0.3687\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3625 - mse: 0.3625\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3637 - mse: 0.3637\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3594 - mse: 0.3594\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3577 - mse: 0.3577\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3548 - mse: 0.3548\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3572 - mse: 0.3572\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3557 - mse: 0.3557\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3507 - mse: 0.3507\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3523 - mse: 0.3523\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3479 - mse: 0.3479\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.3478 - mse: 0.3478\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3483 - mse: 0.3483\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3473 - mse: 0.3473\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3492 - mse: 0.3492\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3435 - mse: 0.3435\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3467 - mse: 0.3467\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3399 - mse: 0.3399\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3415 - mse: 0.3415\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3385 - mse: 0.3385\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3392 - mse: 0.3392\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3406 - mse: 0.3406\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3381 - mse: 0.3381\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3391 - mse: 0.3391\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3343 - mse: 0.3343\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3355 - mse: 0.3355\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3320 - mse: 0.3320\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3317 - mse: 0.3317\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3275 - mse: 0.3275\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3300 - mse: 0.3300\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3274 - mse: 0.3274\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.3268 - mse: 0.3268\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3240 - mse: 0.3240\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3259 - mse: 0.3259\n"
     ]
    }
   ],
   "source": [
    "# Our second model will have three hidden layers.\n",
    "# The first and the second hidden layers will be identical to those of the previous model.\n",
    "# For the third hidden layer, we will set eight neurons on the layer.\n",
    "# The following code defines the second model.\n",
    "\n",
    "# Define the model - deep neural net with two hidden layers\n",
    "number_input_features = 11\n",
    "hidden_nodes_layer1 = 22\n",
    "hidden_nodes_layer2 = 11\n",
    "hidden_nodes_layer3 = 8\n",
    "\n",
    "# Create a sequential neural network model\n",
    "nn_2 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_2.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_2.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Add the third hidden layer\n",
    "nn_2.add(Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# Add the output layer\n",
    "nn_2.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "nn_2.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "\n",
    "# Fit the model\n",
    "deep_net_model_2 = nn_2.fit(X_train_scaled, y_train, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aae930c-2b15-4aec-8400-ed9fb650291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 - 1s - loss: 0.4754 - mse: 0.4754 - 558ms/epoch - 43ms/step\n",
      "13/13 - 0s - loss: 0.4789 - mse: 0.4789 - 344ms/epoch - 26ms/step\n"
     ]
    }
   ],
   "source": [
    "# BREAKDOWN\n",
    "# After compiling and fitting both models, we can assess the impact of adding a third hidden layer by passing each model the testing data and then comparing their mean squared values.\n",
    "# As we've done before, we use the `evalute` function, and pass the testing datasets as parameters, to retrieve the evaluation metrics.\n",
    "# Because we used the `mse` metric when compiling the model, the `evaluate` function returns the value for the `mse` metric as well as the loss function.\n",
    "# The following code evaluates our models.\n",
    "\n",
    "# Evaluate Model 1 using testing data:\n",
    "model1_loss, model1_mse = nn_1.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Evaluate Model 2 using testing data:\n",
    "model2_loss, model2_mse = nn_2.evaluate(X_test_scaled, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ea70f6f-dcc0-4d92-84a0-b627c49d1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BREAKDOWN\n",
    "# As shown in the preceding image, Model 1's MSE is 0.4754, and Model 2's MSE is 0.4789.\n",
    "# These results tell us that the difference between the two models `mse` metrics is quite small.\n",
    "# So, in this case, we can use the simpler model with two hidden layers to make predictions.\n",
    "# Adding layers does not always guarantee better model performance.\n",
    "# Depending on the input data's complexity, adding more hidden lyers sometimes just increases the chance of overfitting the training data.\n",
    "# Unfortunately, no easy solution or rule of thumb exists to identify how many layers will maximize performance for a given model.\n",
    "# Trial and error is the only way to determine how 'deep' a deep learning model should be.\n",
    "# You must train and evaluate a model with deeper and deeper layers, until the model no longer demonstrates noticeable improvements over the same number of epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
